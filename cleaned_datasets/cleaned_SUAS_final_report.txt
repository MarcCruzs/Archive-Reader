suas competition software team fall 2023 final report authors marc cruz abdul kalam syed max gross joshua estrada jason mar josh ng ethan tarrer sarkis gafayan rubayet mujahid david jackson status done date sep 21 2023 relative links suas competition software team overview suas competition technical design document suas competition machine learning models 1 introduction 2 2 fall 2023 progress 3 21 timeline 3 22 recruitment august september 4 23 learning phase august october 4 231 odlc 4 232 obstacle avoidance 5 24 odlc design 5 25 hardware trade studies 7 251 odlc trade studies 7 2511 obc trade study 7 2511 camera trade study 8 252 obstacle avoidance trade study 9 26 data collection and dataset phase october december 10 261 standardized object shape ml model 10 262 standardized object shape alphanumeric color ml model 12 263 standardized object alphanumeric ml model 13 27 modeling phase october february 13 271 uav image recognition trade study decision matrix 13 2711 training yolo models 15 3 reflectionmethod revisions 17 4 conclusionfuture work 17 5 references 18 1 introduction this is a progress report for the software team for suas 2024 this document contains current progress of the project and also adding any new revisions that the suas 2025 software team to consider to increase the odds of the cpp suas team to win more consistently suas or student unmanned aerial systems is a yearly competition with a particular mission for 2024 competition the mission is multiple package delivery companies have tasked uas to deliver packages to customers these uas must avoid each other travel to the customer identify potential drop locations and deliver the package to a safe location the competition is broken down into four mission tasks autonomous flight obstacle avoidance object detection classification localization odlc and air delivery this report focuses on the progress on the implementation of the odlc and obstacle avoidance mission tasks for details of actual methodology please refer to suas competition technical design document for odlc we are required to detect and classify two types of objects standardized objects emergent objects standardized objects are 85 x 11 in size of various shapes alphanumerics color of shape and color of alphanumeric figure 1 standard object left white a on blue triangle and emergent object right manikin dressed in clothes the potential shapes according to the competition rulebook circle semicircle quarter circle triangle rectangle pentagon star and cross the potential colors according to the competition rulebook white black red blue green purple brown and orange these targets are to be detected by the minimum altitude of 75 ft and ideally within the range of 85 90 ft then with the classification of the target to be able to determine localization and signaling payload drop and coordinates to uav for obstacle avoidance creating an effective range around the uav to ensure safety of the drone would be creating the integration and system with a lidar and onboard flight computer to detect potential obstacles and to avoid obstacles accordingly software team members abdul kalam syed joshua estrada josh ng jason mar max gross ethan tarrer sarkis gafafyan marc cruz 2 fall 2023 progress for the suas project there was more consideration of recruitment teaching and actual implementation towards suas mission tasks with that in mind all the information was written into a general timeline with inclusion of methodology designs and trade studies 21 timeline recruitment august september creating the uav lab interview system advertising and interviewing members to join the project learning phase august october introduce machine learning basics cover different types of machine learning models ensure all team members have a solid understanding data collection and dataset phase october december highlight the importance of data collection and preparation create a dataset tailored to suas competition objectives modeling phase october february explain the process of model selection and experimentation encourage team members to explore prebuilt models and consider custom cnn development realtime testing phase february august describe the implementation process for the selected model on the uav emphasize realtime testing and finetuning 22 recruitment august september during this phase focused on overall suas recruitment for each subteam including autonomous flight team payload team uav design team systems team and software team during this phase marc cruz created and oversaw the uav lab interviews where the suas project along with the other projects within uav lab interviewed together to speed up the recruitment process consisted of 5 interviewers sorting through 40 applicants and passing the most optimal members derived from the secretary problem or an adjusted optimal stopping theory geared so that suas has individuals who are capable of finishing the mission tasks or the core members but a larger emphasis on individuals who are able to work on the project for extended time of 23 years to solidify a solid group for future suas competition then once a solid core group is created consideration of recruitment of sophomore or junior standing individuals to learn and later on take over graduating core member positions these individuals are considered the auxiliary members marc cruz although i want to win the competition during my time as software lead i also wanted to ensure that the project has longevity in terms of the knowledge that is accumulated from each years competition previous years lacked proper documentation or available members to transfer information to the current competition year thus the strong emphasis on the recruitment phase as it will determine if suas and the other projects can make substantial progress that ultimately leads to the completion and final product of uav lab projects except in terms of suas faster completion of mission tasks and allow room for even more advanced implementation to be considered 23 learning phase august october 231 odlc as new members are introduced and onboarded each member that was onboarded lack a strong foundation in machine learning or data science with that in mind to mitigate this problem the learning phase was considered and implemented with the help of david jackson the list of libraries to familiarize themselves with relative libraries including numpy opencv and pytorch for numpy it would be good to cover basic operations mathematical functions reshaping arrays combining arrays element wise operations linear algebra possibly optional but good to review array statistics possibly optional but good to review for opencv computer vision library used to clean the image up to pass through to the machine learning library video in and out cuttingslicing portions of images binary image processing thresholding erosion and dilation opening and closing contours blob detection connected component analysis connected blobs for pytorch pytesseract kerasocr machine learning and pretrained text recognition models using pretrained models in pytorch using pytesseract for character recognition using kerasocr for character recognition identifying which has better results for the application at hand 232 obstacle avoidance for obstacle avoidance as the topic is new to software team mimic the usage of other drones within the same lab the implementations that were considered by other teams were the usage of lidar to detect obstacles and sending an output to their pixhawk or whatever hardware managing the flight path and rerouting away or around the obstacle looked into various open source lidar implementations such as mits mader that is capable of generating initial flight path and creating a spline to be followed that goes around the object 24 odlc design during this phase the methodology was further fleshed out and the general design of the multi modal machine learning application an overview of the general consists of multiple machine learning models stacked upon each other to consider multiple traits of the two types of targets odlc is required to find the illustration below illustrates the methodology of how the machine inference system should operate a diagram of different colored circles description automatically generated figure 2 flowchart of the machine inference methodology the machine inferencing methodology consists of 45 machine learning models that correspond to a trait predicting standardized objects shape shape color alphanumeric color and alphanumeric chosen to combine multiple models instead of considerations of a cluster in order to utilize the current software teams knowledge then the illustration below depicts the hardware integration of the machine inference methodology a diagram of a machine inferencing methodology description automatically generated figure 3 illustrates the hardware interactions between obc camera and other equipment on the uav drone 25 hardware trade studies this section contains the various trade studies that were conducted for required components to conduct odlc and obstacle avoidance 251 odlc trade studies 2511 obc trade study obc trade study raspberry pi 5 coral accelerator raspberry pi 4 coral accelerator jetson orin nano criteria weight description grade weighted description grade weighted description grade weighted cost 2 171 3 6 170 3 6 500 1 2 processing units 5 quadcore 64bit arm cortexa76 cpu 1 5 cortexa72 arm v8 64bit soc 18ghz 1 5 1024core nvidia ampere architecture gpu with 32 tensor cores 35 175 software support 1 yes 35 35 yes 35 35 yes 35 35 power consumption w 3 12 2 6 8 3 9 1022 2 6 weight g 1 48 35 35 47 35 35 176 2 2 average grade 26 28 24 weighted total 24 27 31 obc normalization chart grade 1 2 3 35 cost 300 300 200 100 processing units reduced accuracy subpar processing subpar optimal semi optimal processing optimal processing software support software obsolescence software is update to date power consumption w 13 12 8 5 weight g 200 200 100 50 2511 camera trade study camera trade study oakd pro w poe imx378 sensor siyi zr10 rpi hq camera 6mm lens criteria weight description grade weighted description grade weighted description grade weighted cost 1 599 1 1 506 1 1 75 35 35 weight g 1 184 3 3 381 1 1 834 35 35 resolutionfps 5 1080p60fps 4k30fps 35 175 2560 x 1440p30 35 175 2028 1080p50 2028 1520p40 1332 990p120 35 175 power consumption w 3 2w 55 w 3 9 3 w 3 9 14 w 35 105 fov 4 hfov 95 vfov 70 35 14 hfovl 715 dfov 795 3 12 63 3 12 average grade 28 23 34 weighted total 445 405 47 camera normalization chart grade 1 2 3 35 cost 400 400 250 250 100 100 weight g 350 350 200 200 50 50 resolutionfps incapable of high resolution 30 fps high resolution 30 fps power consumption w 10 10 5 5 2 2 fov subpar optimal fov moderately optimal fov strongly optimal fov 252 obstacle avoidance trade study lidar trade study rplidar a2m8 livox mid40 rplidar a3 criteria weight description grade weighted description grade weighted description grade weighted cost 1 319 3 3 599 3 3 599 3 3 detection range m 5 01 16 35 175 260 35 175 8 2 10 software support 3 yes 35 105 yes 35 105 yes 35 105 power consumption w 4 3 3 12 10 1 4 3 3 12 weight g 2 190 2 4 760 1 2 190 2 4 average grade 3 24 27 weighted total 47 37 395 lidar normalization chart grade 1 2 3 35 cost 800 800 400 200 range m 6 8 12 12 software support software obsolescence software is update to date power consumption w 5 4 3 2 weight g 200 200 150 100 26 data collection and dataset phase october december during this phase members focused on working the odlc mission task were split between all the models and focused on researching and collecting the required data for their specific machine learning model 261 standardized object shape ml model for the data collection for detecting shapes of the standardized object the nature of the targets unique characteristics lead to the need of collecting its own dataset due to the difficulty of needing a drone pilot with licensing and limited scheduling with current licensed drone pilots led to consideration of synthetic dataset that combines aerial image datasets 1234 and scripting potential variations of standardized targets onto each image in figure 4 it depicts some of the aerial images from the dataset although these datasets are not the ideal conditions we would be competing in due to the current circumstances the model will rely on the datasets that have over 100000 images in just one of the several considered datasets then for the environment of the aerial images is still addresses competition environment as the competition environment is runway blacktop that has the same characteristics of the roads in the aerial image datasets a collage of a road description automatically generated in figure 4 various samples from the aerial datasets starting with one of the datasets 3 began experimenting scripting standardized targets onto the images and made it so that the images were within a certain size percentage due to the nature of the dataset not having a consistent altitude it was determined to create relative sized images of the standardized targets based on the rulebook several samples depicted below figure 5 first created samples of experimental scripting of standardized objects onto aerial images to test the validity of the synthetic dataset collected we trained yolov5 on the custom dataset the model was only trained to detect circles in different colors with colored alphanumeric the model was trained on 750 training images plus 500 validation images out of 24000 images collected for the circle and the results were promising as shown in figure 6 the trained model can detect very small circles with consistency however it is not very accurate as it recognizes any round object as a circle the confusion matrix had a score of 100 which should not have been the case as it still doesnt detect circles of different sizes the cause of the confusion matrix being so high is possibly overfitting the training model and validation did not contain the same images but they were similar introducing more shapes should make the model more accurate as well figure 6 trained yolov5 model results being able to detect small colored circles with colored alphanumeric the scripting of images is currently experimental and more work would be completed during the winter break further work mentioned in future work section 262 standardized object shape alphanumeric color ml model before creation of the color ml model focused on preprocessing images in order to increase the odds of predicting the color of the shape and alphanumeric colors with the given list of colors from the rulebook we are taking an image of a segmented predicted shape that is a result from the standardized object shape model as the image being segmented the images majority colors should be of the shape color and alphanumeric color with that in mind we extract rgb values of the shape and alphanumeric or the 2 most common values in the image this is the approach we chose to determine the rgb values of the image then we move onto various methods of making a model the two approaches are using the finite range rgb values 0255 and make a range for each rgb value that determines if it is a certain color for example it is known that 000 is black or 255255255 is white then the second approach is creating or finding a rgb value dataset that has various rgb values that are labeled to its corresponding color then by crossreferencing the pixel values to their rgb equivalentthe color that is most prevalent in the picture will be used to determine overall color using this same strategy the color of the alphanumeric character can also be deduced by further shrinking the bounding box 263 standardized object alphanumeric ml model with using opencv ocr to be able to conduct text detection that allows the alphanumeric to be read and output data collection was not required but reimplementation of the opencv ocr was the main focus for this model as it is already prebuilt to do so using the same segmented image given to the color ml model the image is taken as an input and outputs the text in the given image this model acts as the main method of ensuring no false positives as occurences of a nonstandardized target being a certain shape would not be considered a target if no text can be derived from the image with a high confidence threshold 90 confidence the threshold may change depending on future experimentation in the spring semester 27 modeling phase october february due to the software teams lack of expertise in the subject of object detection models and the preprocessing of images related led to need to research information on implementation of various machine learning models 271 uav image recognition trade study decision matrix methodologies under study convolutional neural networks cnns faster rcnn yolo you only look once criteria for evaluation accuracy ability to correctly identify and classify objects speed realtime processing capability resource requirement computational and memory demand training data dependency amount of data required for effective training implementation complexity difficulty in integrating with uav systems methodology analysis 1 convolutional neural networks cnns description hierarchical deep learning models specialized for image and pattern recognition tasks pros layered architecture enables feature extraction at multiple levels from simple to complex transfer learning pretrained models available which can be finetuned for specific tasks adaptable can be designed for varied complexities from simple to very deep networks robustness performs well even with minor image distortions or variations cons overfitting prone to overfitting especially with limited data computational intensity requires powerful gpus for training and inference training time deep cnns can take substantial time to train interpretability difficult to interpret and understand layerwise operations research references 5 6 2 faster rcnn description advanced cnnbased model tailored for realtime object detection classification and localization pros region proposal network efficiently suggests potential object locations endtoend training simplifies the training process by merging steps high precision capable of detecting multiple objects with high accuracy flexible can be integrated with other architectures like resnet cons complexity multiple components add to the complexity of the architecture memory consumption utilizes more memory due to region proposals slower inference not as fast as singleshot detectors like yolo requires more labeled data for accurate region proposal and detection research references 7 8 3 yolo you only look once description ultrafast object detection system that treats detection as a regression problem pros speed designed for realtime applications with impressive fps simplicity singleshot detector without region proposals unified model detects and classifies in one forward pass scalable variants available eg tinyyolo for devices with limited computational capabilities cons precision may have reduced accuracy compared to twostage methods like faster rcnn spatial constraints due to grid system struggles with small object detection overlapping objects might struggle when objects overlap substantially limited bounding boxes predicts a fixed number of bounding boxes based on grid size research references 9 10 comparison table criteria cnns faster rcnn yolo accuracy high very high high speed medium medium very high resource requirement high very high medium training data dependency high high medium implementation complexity medium medium low conclusion given the requirements of the uav competition which include realtime processing limited computational resources onboard the uav and the need for high accuracy the tradeoff between speed and accuracy becomes paramount recommendation begin with faster rcnn due to its balance of speed and accuracy if computational resources or implementation prove to be significant barriers consider transitioning to yolo for its efficiency and realtime capabilities 2711 training yolo models to train custom dataset on yolov5 we need an image file png or jpeg and a txt file that contains the class value xcoordinate and ycoordinate of the center of the target and the width and height of the target yolo does not care about the bounding box drawn on the images it just needs the annotation for each image the coordinates width and height all need to be normalized to be between 0 and 1 the way to do it is dividing the xcoordinate and the width with xmax and the ycoordinate and height with ymax where xmax is the total width of the image minus one and ymax is the total height of the image minus 1 so if we have an image of size 100 pxl x 200 pxl and the center of the bounding box at xy 2677 and width and height of the bounding box is wh 56 then we will divide 26 by 99 and 5 by 99 as well and we will divide 77 by 199 and 6 by 199 as well we call these annotations and it will look something like class this is the class it is the class number that this image has in the picture class number must start from 0 if not you will have to add that many empty values in the yaml file which we will discuss later xcoordinate ycoordinate width height ex 0 0815 0777 00345 00392 this phase was only for testing to make sure that we generate proper data that can be used to train yolo we will be creating more data and training yolo in the next few weeks for now we only created all the possible color and alphanumeric combinations of circle one of the shapes 8 shapes we need to be able to recognize the dataset came out to be about 24000 images plus an additional 24000 annotations for those images however 23000 images is a lot and would take a lot of time for just testing purposes so trained the model on 750 training images plus 500 validation images good time to mention how yolo takes the data to train to train yolo on custom dataset you need to divide the dataset into three sections by create three folders within the dataset folder train val and test train and val are mandatory test can be ignored but good practice is to have it each with two subfolders images and labels the train folder will contain the most images and corresponding annotations the val validation folder will contain less data but it will be validated after every epochs number of times to train the model on the training dataset and lastly test will contain even less data but will be ran for testing at the end it is important to note that images in all these folders should be different especially train and val we want to validate the trained model on a unique set of images so we can increase its accuracy we will also need a yaml file that will contain the location of each folder train val and test in addition to number of classes and the names of classes it would look something like this in the example we can see that circle is placed on position 7 start from position 0 of the array because in our annotations we set the class to 7 instead of 0 either way we will place this file in the data folder which is within our yolov5 cloned folder we need to place the dataset folder in the yolov5 cloned folder as well to start training you need to navigate to the yolov5 cloned folder in command prompt and then place this command python trainpy imgsize 640 batchsize 16 epochs 50 data datayour_yaml_fileyaml weights yolov5spt the yolo model can now detect very small circles on a picture the training time for 1250 images was more than 5 hours and we have a dataset of more than 24000 images per each class training the model on this sized dataset which we should increase accuracy will require some high specs especially a good gpu however there is a bit of a problem with the dataset which we need to fix even though we were able to train the model and it can detect small images it is not detecting large circles the next step is to analyze the results and look for imperfections there are results that yolo generates that we can use to analyze the results 3 reflectionmethod revisions various reflections and method revisions consider nelson brown armstrong engineer who is currently working on dataset collected in a variety of altitudes but may have aerial images that might be within the ideal images for data collection due to the recruitment phase not starting earlier in the summer it led to a reduced amount of time to effectively work on the project and the software team had to deal with recruitment and retention of members in the beginning of the semester the lack of expertise in the team was attempted to be remedied by overviewing the relevant libraries but still needed more fundamental understanding although there are classes that teach machine learning applications it does not say for certain that it gives the right fundamentals for the projects scope as the classes attend to teach the overarching domain of machine learning applications and not specifically into computer vision and object detection classification 4 conclusionfuture work during the fall semester was able to recruit and get members to have a basic understanding of machine learning and libraries that are used to implement a machine learning model due to the group having only a simple understanding of ml and no experience in machine inference implementation this semester was mainly focused on research and data collection for the models was able to determine what is going to be used for data collection for standardized target shape color of alphanumeric shape and the alphanumeric then using yolo architecture for the shape model to have fast inference time in order to do real time machine inference then the shape model will likely use yolo series but remains to be determined during the winter break for the alphanumeric predicting model would be using opencv ocr to detect text and output the found texts for obstacle avoidance it was pure research for the semester as knowledge and implementation is new and as of what is known for curriculum is only within the grad student classes for future work once data collection has been completed and the models for each dataset have been created would need to make the models to be able to run on a jetson orin nano developer kit would attempt to turn the models into tensorflow lite models or equivalent form to be ran on the developer kit benchmarks of the flops of the models and the various settings of the developer kit ai performance would be tested and implemented then to fix issues of lack of expertise for future suas software members would begin recruitment in spring 2024 to allow new members to have more time to familiarize themselves with the work and be capable of contributing in fall 2024 also reduces the need for recruitment for the next suas competition and utilizes the full semester to work rather than recruiting again then also consider making a whole team entirely designated for obstacle avoidance as the scope of team leader becomes too wide to cover all bases when asked to have specialty in two different topics 5 references 1 httpssitesgooglecomviewgrliuavdt首页 2 httpsvisioneeccuedutwaerialimage 3 httpscemsekaustedusaivuluav123 4 httpswwwproquestcomdocview2530133784fulltextpdf8a0658d0d93f45c0pq1accountid10357 5 a krizhevsky i sutskever and g e hinton imagenet classification with deep convolutional neural networks in proc advances in neural inf process systems 2012 pp 10971105 6 y lecun y bengio and g hinton deep learning nature vol 521 no 7553 pp 436444 2015 7 s ren k he r girshick and j sun faster rcnn towards realtime object detection with region proposal networks in proc advances in neural inf process systems 2015 pp 9199 8 k he x zhang s ren and j sun deep residual learning for image recognition in proc ieee conf computer vision pattern recognition 2016 9 j redmon s divvala r girshick and a farhadi you only look once unified realtime object detection in proc ieee conf computer vision pattern recognition 2016 pp 779788 10 j redmon and a farhadi yolo9000 better faster stronger in proc ieee conf computer vision pattern recognition 2017