suas competition software team fall two thousand and twenty-three final report authors marc cruz abdul kalam syed max gross joshua estrada jason mar josh ng ethan tarrer sarkis gafayan rubayet mujahid david jackson status done date sep twenty-one two thousand and twenty-three relative links suas competition software team overview suas competition technical design document suas competition machine learning models one introduction two two fall two thousand and twenty-three progress three twenty-one timeline three twenty-two recruitment august september four twenty-three learning phase august october four two hundred and thirty-one odlc four two hundred and thirty-two obstacle avoidance five twenty-four odlc design five twenty-five hardware trade studies seven two hundred and fifty-one odlc trade studies seven two thousand, five hundred and eleven obc trade study seven two thousand, five hundred and eleven camera trade study eight two hundred and fifty-two obstacle avoidance trade study nine twenty-six data collection and dataset phase october december ten two hundred and sixty-one standardized object shape ml model ten two hundred and sixty-two standardized object shape alphanumeric color ml model twelve two hundred and sixty-three standardized object alphanumeric ml model thirteen twenty-seven modeling phase october february thirteen two hundred and seventy-one uav image recognition trade study decision matrix thirteen two thousand, seven hundred and eleven training yolo models fifteen three reflectionmethod revisions seventeen four conclusionfuture work seventeen five references eighteen one introduction this is a progress report for the software team for suas two thousand and twenty-four this document contains current progress of the project and also adding any new revisions that the suas two thousand and twenty-five software team to consider to increase the odds of the cpp suas team to win more consistently suas or student unmanned aerial systems is a yearly competition with a particular mission for two thousand and twenty-four competition the mission is multiple package delivery companies have tasked uas to deliver packages to customers these uas must avoid each other travel to the customer identify potential drop locations and deliver the package to a safe location the competition is broken down into four mission tasks autonomous flight obstacle avoidance object detection classification localization odlc and air delivery this report focuses on the progress on the implementation of the odlc and obstacle avoidance mission tasks for details of actual methodology please refer to suas competition technical design document for odlc we are required to detect and classify two types of objects standardized objects emergent objects standardized objects are eighty-five x eleven in size of various shapes alphanumerics color of shape and color of alphanumeric figure one standard object left white a on blue triangle and emergent object right manikin dressed in clothes the potential shapes according to the competition rulebook circle semicircle quarter circle triangle rectangle pentagon star and cross the potential colors according to the competition rulebook white black red blue green purple brown and orange these targets are to be detected by the minimum altitude of seventy-five ft and ideally within the range of eighty-five ninety ft then with the classification of the target to be able to determine localization and signaling payload drop and coordinates to uav for obstacle avoidance creating an effective range around the uav to ensure safety of the drone would be creating the integration and system with a lidar and onboard flight computer to detect potential obstacles and to avoid obstacles accordingly software team members abdul kalam syed joshua estrada josh ng jason mar max gross ethan tarrer sarkis gafafyan marc cruz two fall two thousand and twenty-three progress for the suas project there was more consideration of recruitment teaching and actual implementation towards suas mission tasks with that in mind all the information was written into a general timeline with inclusion of methodology designs and trade studies twenty-one timeline recruitment august september creating the uav lab interview system advertising and interviewing members to join the project learning phase august october introduce machine learning basics cover different types of machine learning models ensure all team members have a solid understanding data collection and dataset phase october december highlight the importance of data collection and preparation create a dataset tailored to suas competition objectives modeling phase october february explain the process of model selection and experimentation encourage team members to explore prebuilt models and consider custom cnn development realtime testing phase february august describe the implementation process for the selected model on the uav emphasize realtime testing and finetuning twenty-two recruitment august september during this phase focused on overall suas recruitment for each subteam including autonomous flight team payload team uav design team systems team and software team during this phase marc cruz created and oversaw the uav lab interviews where the suas project along with the other projects within uav lab interviewed together to speed up the recruitment process consisted of five interviewers sorting through forty applicants and passing the most optimal members derived from the secretary problem or an adjusted optimal stopping theory geared so that suas has individuals who are capable of finishing the mission tasks or the core members but a larger emphasis on individuals who are able to work on the project for extended time of twenty-three years to solidify a solid group for future suas competition then once a solid core group is created consideration of recruitment of sophomore or junior standing individuals to learn and later on take over graduating core member positions these individuals are considered the auxiliary members marc cruz although i want to win the competition during my time as software lead i also wanted to ensure that the project has longevity in terms of the knowledge that is accumulated from each years competition previous years lacked proper documentation or available members to transfer information to the current competition year thus the strong emphasis on the recruitment phase as it will determine if suas and the other projects can make substantial progress that ultimately leads to the completion and final product of uav lab projects except in terms of suas faster completion of mission tasks and allow room for even more advanced implementation to be considered twenty-three learning phase august october two hundred and thirty-one odlc as new members are introduced and onboarded each member that was onboarded lack a strong foundation in machine learning or data science with that in mind to mitigate this problem the learning phase was considered and implemented with the help of david jackson the list of libraries to familiarize themselves with relative libraries including numpy opencv and pytorch for numpy it would be good to cover basic operations mathematical functions reshaping arrays combining arrays element wise operations linear algebra possibly optional but good to review array statistics possibly optional but good to review for opencv computer vision library used to clean the image up to pass through to the machine learning library video in and out cuttingslicing portions of images binary image processing thresholding erosion and dilation opening and closing contours blob detection connected component analysis connected blobs for pytorch pytesseract kerasocr machine learning and pretrained text recognition models using pretrained models in pytorch using pytesseract for character recognition using kerasocr for character recognition identifying which has better results for the application at hand two hundred and thirty-two obstacle avoidance for obstacle avoidance as the topic is new to software team mimic the usage of other drones within the same lab the implementations that were considered by other teams were the usage of lidar to detect obstacles and sending an output to their pixhawk or whatever hardware managing the flight path and rerouting away or around the obstacle looked into various open source lidar implementations such as mits mader that is capable of generating initial flight path and creating a spline to be followed that goes around the object twenty-four odlc design during this phase the methodology was further fleshed out and the general design of the multi modal machine learning application an overview of the general consists of multiple machine learning models stacked upon each other to consider multiple traits of the two types of targets odlc is required to find the illustration below illustrates the methodology of how the machine inference system should operate a diagram of different colored circles description automatically generated figure two flowchart of the machine inference methodology the machine inferencing methodology consists of forty-five machine learning models that correspond to a trait predicting standardized objects shape shape color alphanumeric color and alphanumeric chosen to combine multiple models instead of considerations of a cluster in order to utilize the current software teams knowledge then the illustration below depicts the hardware integration of the machine inference methodology a diagram of a machine inferencing methodology description automatically generated figure three illustrates the hardware interactions between obc camera and other equipment on the uav drone twenty-five hardware trade studies this section contains the various trade studies that were conducted for required components to conduct odlc and obstacle avoidance two hundred and fifty-one odlc trade studies two thousand, five hundred and eleven obc trade study obc trade study raspberry pi five coral accelerator raspberry pi four coral accelerator jetson orin nano criteria weight description grade weighted description grade weighted description grade weighted cost two one hundred and seventy-one three six one hundred and seventy three six five hundred one two processing units five quadcore 64bit arm cortexa76 cpu one five cortexa72 arm v8 64bit soc 18ghz one five 1024core nvidia ampere architecture gpu with thirty-two tensor cores thirty-five one hundred and seventy-five software support one yes thirty-five thirty-five yes thirty-five thirty-five yes thirty-five thirty-five power consumption w three twelve two six eight three nine one thousand and twenty-two two six weight g one forty-eight thirty-five thirty-five forty-seven thirty-five thirty-five one hundred and seventy-six two two average grade twenty-six twenty-eight twenty-four weighted total twenty-four twenty-seven thirty-one obc normalization chart grade one two three thirty-five cost three hundred three hundred two hundred one hundred processing units reduced accuracy subpar processing subpar optimal semi optimal processing optimal processing software support software obsolescence software is update to date power consumption w thirteen twelve eight five weight g two hundred two hundred one hundred fifty two thousand, five hundred and eleven camera trade study camera trade study oakd pro w poe imx378 sensor siyi zr10 rpi hq camera 6mm lens criteria weight description grade weighted description grade weighted description grade weighted cost one five hundred and ninety-nine one one five hundred and six one one seventy-five thirty-five thirty-five weight g one one hundred and eighty-four three three three hundred and eighty-one one one eight hundred and thirty-four thirty-five thirty-five resolutionfps five 1080p60fps 4k30fps thirty-five one hundred and seventy-five two thousand, five hundred and sixty x 1440p30 thirty-five one hundred and seventy-five two thousand and twenty-eight 1080p50 two thousand and twenty-eight 1520p40 one thousand, three hundred and thirty-two 990p120 thirty-five one hundred and seventy-five power consumption w three 2w fifty-five w three nine three w three nine fourteen w thirty-five one hundred and five fov four hfov ninety-five vfov seventy thirty-five fourteen hfovl seven hundred and fifteen dfov seven hundred and ninety-five three twelve sixty-three three twelve average grade twenty-eight twenty-three thirty-four weighted total four hundred and forty-five four hundred and five forty-seven camera normalization chart grade one two three thirty-five cost four hundred four hundred two hundred and fifty two hundred and fifty one hundred one hundred weight g three hundred and fifty three hundred and fifty two hundred two hundred fifty fifty resolutionfps incapable of high resolution thirty fps high resolution thirty fps power consumption w ten ten five five two two fov subpar optimal fov moderately optimal fov strongly optimal fov two hundred and fifty-two obstacle avoidance trade study lidar trade study rplidar a2m8 livox mid40 rplidar a3 criteria weight description grade weighted description grade weighted description grade weighted cost one three hundred and nineteen three three five hundred and ninety-nine three three five hundred and ninety-nine three three detection range m five one sixteen thirty-five one hundred and seventy-five two hundred and sixty thirty-five one hundred and seventy-five eight two ten software support three yes thirty-five one hundred and five yes thirty-five one hundred and five yes thirty-five one hundred and five power consumption w four three three twelve ten one four three three twelve weight g two one hundred and ninety two four seven hundred and sixty one two one hundred and ninety two four average grade three twenty-four twenty-seven weighted total forty-seven thirty-seven three hundred and ninety-five lidar normalization chart grade one two three thirty-five cost eight hundred eight hundred four hundred two hundred range m six eight twelve twelve software support software obsolescence software is update to date power consumption w five four three two weight g two hundred two hundred one hundred and fifty one hundred twenty-six data collection and dataset phase october december during this phase members focused on working the odlc mission task were split between all the models and focused on researching and collecting the required data for their specific machine learning model two hundred and sixty-one standardized object shape ml model for the data collection for detecting shapes of the standardized object the nature of the targets unique characteristics lead to the need of collecting its own dataset due to the difficulty of needing a drone pilot with licensing and limited scheduling with current licensed drone pilots led to consideration of synthetic dataset that combines aerial image datasets one thousand, two hundred and thirty-four and scripting potential variations of standardized targets onto each image in figure four it depicts some of the aerial images from the dataset although these datasets are not the ideal conditions we would be competing in due to the current circumstances the model will rely on the datasets that have over one hundred thousand images in just one of the several considered datasets then for the environment of the aerial images is still addresses competition environment as the competition environment is runway blacktop that has the same characteristics of the roads in the aerial image datasets a collage of a road description automatically generated in figure four various samples from the aerial datasets starting with one of the datasets three began experimenting scripting standardized targets onto the images and made it so that the images were within a certain size percentage due to the nature of the dataset not having a consistent altitude it was determined to create relative sized images of the standardized targets based on the rulebook several samples depicted below figure five first created samples of experimental scripting of standardized objects onto aerial images to test the validity of the synthetic dataset collected we trained yolov5 on the custom dataset the model was only trained to detect circles in different colors with colored alphanumeric the model was trained on seven hundred and fifty training images plus five hundred validation images out of twenty-four thousand images collected for the circle and the results were promising as shown in figure six the trained model can detect very small circles with consistency however it is not very accurate as it recognizes any round object as a circle the confusion matrix had a score of one hundred which should not have been the case as it still doesnt detect circles of different sizes the cause of the confusion matrix being so high is possibly overfitting the training model and validation did not contain the same images but they were similar introducing more shapes should make the model more accurate as well figure six trained yolov5 model results being able to detect small colored circles with colored alphanumeric the scripting of images is currently experimental and more work would be completed during the winter break further work mentioned in future work section two hundred and sixty-two standardized object shape alphanumeric color ml model before creation of the color ml model focused on preprocessing images in order to increase the odds of predicting the color of the shape and alphanumeric colors with the given list of colors from the rulebook we are taking an image of a segmented predicted shape that is a result from the standardized object shape model as the image being segmented the images majority colors should be of the shape color and alphanumeric color with that in mind we extract rgb values of the shape and alphanumeric or the two most common values in the image this is the approach we chose to determine the rgb values of the image then we move onto various methods of making a model the two approaches are using the finite range rgb values two hundred and fifty-five and make a range for each rgb value that determines if it is a certain color for example it is known that zero is black or two hundred and fifty-five million, two hundred and fifty-five thousand, two hundred and fifty-five is white then the second approach is creating or finding a rgb value dataset that has various rgb values that are labeled to its corresponding color then by crossreferencing the pixel values to their rgb equivalentthe color that is most prevalent in the picture will be used to determine overall color using this same strategy the color of the alphanumeric character can also be deduced by further shrinking the bounding box two hundred and sixty-three standardized object alphanumeric ml model with using opencv ocr to be able to conduct text detection that allows the alphanumeric to be read and output data collection was not required but reimplementation of the opencv ocr was the main focus for this model as it is already prebuilt to do so using the same segmented image given to the color ml model the image is taken as an input and outputs the text in the given image this model acts as the main method of ensuring no false positives as occurences of a nonstandardized target being a certain shape would not be considered a target if no text can be derived from the image with a high confidence threshold ninety confidence the threshold may change depending on future experimentation in the spring semester twenty-seven modeling phase october february due to the software teams lack of expertise in the subject of object detection models and the preprocessing of images related led to need to research information on implementation of various machine learning models two hundred and seventy-one uav image recognition trade study decision matrix methodologies under study convolutional neural networks cnns faster rcnn yolo you only look once criteria for evaluation accuracy ability to correctly identify and classify objects speed realtime processing capability resource requirement computational and memory demand training data dependency amount of data required for effective training implementation complexity difficulty in integrating with uav systems methodology analysis one convolutional neural networks cnns description hierarchical deep learning models specialized for image and pattern recognition tasks pros layered architecture enables feature extraction at multiple levels from simple to complex transfer learning pretrained models available which can be finetuned for specific tasks adaptable can be designed for varied complexities from simple to very deep networks robustness performs well even with minor image distortions or variations cons overfitting prone to overfitting especially with limited data computational intensity requires powerful gpus for training and inference training time deep cnns can take substantial time to train interpretability difficult to interpret and understand layerwise operations research references five six two faster rcnn description advanced cnnbased model tailored for realtime object detection classification and localization pros region proposal network efficiently suggests potential object locations endtoend training simplifies the training process by merging steps high precision capable of detecting multiple objects with high accuracy flexible can be integrated with other architectures like resnet cons complexity multiple components add to the complexity of the architecture memory consumption utilizes more memory due to region proposals slower inference not as fast as singleshot detectors like yolo requires more labeled data for accurate region proposal and detection research references seven eight three yolo you only look once description ultrafast object detection system that treats detection as a regression problem pros speed designed for realtime applications with impressive fps simplicity singleshot detector without region proposals unified model detects and classifies in one forward pass scalable variants available eg tinyyolo for devices with limited computational capabilities cons precision may have reduced accuracy compared to twostage methods like faster rcnn spatial constraints due to grid system struggles with small object detection overlapping objects might struggle when objects overlap substantially limited bounding boxes predicts a fixed number of bounding boxes based on grid size research references nine ten comparison table criteria cnns faster rcnn yolo accuracy high very high high speed medium medium very high resource requirement high very high medium training data dependency high high medium implementation complexity medium medium low conclusion given the requirements of the uav competition which include realtime processing limited computational resources onboard the uav and the need for high accuracy the tradeoff between speed and accuracy becomes paramount recommendation begin with faster rcnn due to its balance of speed and accuracy if computational resources or implementation prove to be significant barriers consider transitioning to yolo for its efficiency and realtime capabilities two thousand, seven hundred and eleven training yolo models to train custom dataset on yolov5 we need an image file png or jpeg and a txt file that contains the class value xcoordinate and ycoordinate of the center of the target and the width and height of the target yolo does not care about the bounding box drawn on the images it just needs the annotation for each image the coordinates width and height all need to be normalized to be between zero and one the way to do it is dividing the xcoordinate and the width with xmax and the ycoordinate and height with ymax where xmax is the total width of the image minus one and ymax is the total height of the image minus one so if we have an image of size one hundred pxl x two hundred pxl and the center of the bounding box at xy two thousand, six hundred and seventy-seven and width and height of the bounding box is wh fifty-six then we will divide twenty-six by ninety-nine and five by ninety-nine as well and we will divide seventy-seven by one hundred and ninety-nine and six by one hundred and ninety-nine as well we call these annotations and it will look something like class this is the class it is the class number that this image has in the picture class number must start from zero if not you will have to add that many empty values in the yaml file which we will discuss later xcoordinate ycoordinate width height ex zero eight hundred and fifteen seven hundred and seventy-seven three hundred and forty-five three hundred and ninety-two this phase was only for testing to make sure that we generate proper data that can be used to train yolo we will be creating more data and training yolo in the next few weeks for now we only created all the possible color and alphanumeric combinations of circle one of the shapes eight shapes we need to be able to recognize the dataset came out to be about twenty-four thousand images plus an additional twenty-four thousand annotations for those images however twenty-three thousand images is a lot and would take a lot of time for just testing purposes so trained the model on seven hundred and fifty training images plus five hundred validation images good time to mention how yolo takes the data to train to train yolo on custom dataset you need to divide the dataset into three sections by create three folders within the dataset folder train val and test train and val are mandatory test can be ignored but good practice is to have it each with two subfolders images and labels the train folder will contain the most images and corresponding annotations the val validation folder will contain less data but it will be validated after every epochs number of times to train the model on the training dataset and lastly test will contain even less data but will be ran for testing at the end it is important to note that images in all these folders should be different especially train and val we want to validate the trained model on a unique set of images so we can increase its accuracy we will also need a yaml file that will contain the location of each folder train val and test in addition to number of classes and the names of classes it would look something like this in the example we can see that circle is placed on position seven start from position zero of the array because in our annotations we set the class to seven instead of zero either way we will place this file in the data folder which is within our yolov5 cloned folder we need to place the dataset folder in the yolov5 cloned folder as well to start training you need to navigate to the yolov5 cloned folder in command prompt and then place this command python trainpy imgsize six hundred and forty batchsize sixteen epochs fifty data datayour_yaml_fileyaml weights yolov5spt the yolo model can now detect very small circles on a picture the training time for one thousand, two hundred and fifty images was more than five hours and we have a dataset of more than twenty-four thousand images per each class training the model on this sized dataset which we should increase accuracy will require some high specs especially a good gpu however there is a bit of a problem with the dataset which we need to fix even though we were able to train the model and it can detect small images it is not detecting large circles the next step is to analyze the results and look for imperfections there are results that yolo generates that we can use to analyze the results three reflectionmethod revisions various reflections and method revisions consider nelson brown armstrong engineer who is currently working on dataset collected in a variety of altitudes but may have aerial images that might be within the ideal images for data collection due to the recruitment phase not starting earlier in the summer it led to a reduced amount of time to effectively work on the project and the software team had to deal with recruitment and retention of members in the beginning of the semester the lack of expertise in the team was attempted to be remedied by overviewing the relevant libraries but still needed more fundamental understanding although there are classes that teach machine learning applications it does not say for certain that it gives the right fundamentals for the projects scope as the classes attend to teach the overarching domain of machine learning applications and not specifically into computer vision and object detection classification four conclusionfuture work during the fall semester was able to recruit and get members to have a basic understanding of machine learning and libraries that are used to implement a machine learning model due to the group having only a simple understanding of ml and no experience in machine inference implementation this semester was mainly focused on research and data collection for the models was able to determine what is going to be used for data collection for standardized target shape color of alphanumeric shape and the alphanumeric then using yolo architecture for the shape model to have fast inference time in order to do real time machine inference then the shape model will likely use yolo series but remains to be determined during the winter break for the alphanumeric predicting model would be using opencv ocr to detect text and output the found texts for obstacle avoidance it was pure research for the semester as knowledge and implementation is new and as of what is known for curriculum is only within the grad student classes for future work once data collection has been completed and the models for each dataset have been created would need to make the models to be able to run on a jetson orin nano developer kit would attempt to turn the models into tensorflow lite models or equivalent form to be ran on the developer kit benchmarks of the flops of the models and the various settings of the developer kit ai performance would be tested and implemented then to fix issues of lack of expertise for future suas software members would begin recruitment in spring two thousand and twenty-four to allow new members to have more time to familiarize themselves with the work and be capable of contributing in fall two thousand and twenty-four also reduces the need for recruitment for the next suas competition and utilizes the full semester to work rather than recruiting again then also consider making a whole team entirely designated for obstacle avoidance as the scope of team leader becomes too wide to cover all bases when asked to have specialty in two different topics five references one httpssitesgooglecomviewgrliuavdt首页 two httpsvisioneeccuedutwaerialimage three httpscemsekaustedusaivuluav123 four httpswwwproquestcomdocview2530133784fulltextpdf8a0658d0d93f45c0pq1accountid10357 five a krizhevsky i sutskever and g e hinton imagenet classification with deep convolutional neural networks in proc advances in neural inf process systems two thousand and twelve pp ten million, nine hundred and seventy-one thousand, one hundred and five six y lecun y bengio and g hinton deep learning nature vol five hundred and twenty-one no seven thousand, five hundred and fifty-three pp four hundred and thirty-six thousand, four hundred and forty-four two thousand and fifteen seven s ren k he r girshick and j sun faster rcnn towards realtime object detection with region proposal networks in proc advances in neural inf process systems two thousand and fifteen pp nine thousand, one hundred and ninety-nine eight k he x zhang s ren and j sun deep residual learning for image recognition in proc ieee conf computer vision pattern recognition two thousand and sixteen nine j redmon s divvala r girshick and a farhadi you only look once unified realtime object detection in proc ieee conf computer vision pattern recognition two thousand and sixteen pp seven hundred and seventy-nine thousand, seven hundred and eighty-eight ten j redmon and a farhadi yolo9000 better faster stronger in proc ieee conf computer vision pattern recognition two thousand and seventeen