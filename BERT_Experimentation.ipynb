{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "974edbba-7397-47de-a925-d7020c2ecf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/miniconda3/envs/ArchiveReader/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from DataGeneration import DataGeneration  # Importing the DataGeneration package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3381e1bb-30ce-4a8b-8c4c-b919635dbf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895c9aa3-f33a-445b-839e-c8149969c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/miniconda3/envs/ArchiveReader/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[nltk_data] Downloading package punkt to /home/marc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/marc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and pre-trained BERT model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Instantiate DataGeneration class\n",
    "data_generator = DataGeneration()\n",
    "\n",
    "# Generate data\n",
    "raw_data_file = \"raw_datasets\\SUAS-Competition-FALL2023-Final-Report.txt\"\n",
    "raw_data_file_linux = \"raw_datasets/SUAS-Competition-FALL2023-Final-Report.txt\"\n",
    "train_contexts, train_questions, train_answers = data_generator.generate_data(raw_data_file_linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a08f2b-c0ab-46c3-bbc8-0b421b28e7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['suas competition software team fall 2023 final report authors marc cruz abdul kalam syed max gross joshua estrada jason mar josh ng ethan tarrer sarkis gafayan rubayet mujahid david jackson status done date sep 21 2023 relative links suas competition software team overview suas competition technical design document suas competition machine learning models 1. introduction two 2. fall 2023 progress three 2.1 timeline three 2.2 recruitment august september four 2.3 learning phase august october four 2.3.1 odlc four 2.3.2 obstacle avoidance five 2.4 odlc design five 2.5 hardware trade studies seven 2.5.1 odlc trade studies seven 2.5.1.1 obc trade study seven 2.5.1.1 camera trade study eight 2.5.2 obstacle avoidance trade study nine 2.6 data collection dataset phase october december ten 2.6.1 standardized object shape ml model ten 2.6.2 standardized object shape alphanumeric color ml model twelve 2.6.3 standardized object alphanumeric ml model thirteen 2.7 modeling phase october february thirteen 2.7.1 uav image recognition trade study decision matrix thirteen 2.7.1.1 training yolo models fifteen 3. reflectionmethod revisions seventeen 4. conclusionfuture work seventeen 5. references eighteen 1. introduction progress report software team suas 2024. document contains current progress project also adding new revisions suas 2025 software team consider increase odds cpp suas team win consistently . software team members abdul kalam syed joshua estrada josh ng jason mar max gross ethan tarrer sarkis gafafyan marc cruz 2. fall 2023 progress suas project consideration recruitment teaching actual implementation towards suas mission tasks .', 'suas student unmanned aerial systems yearly competition particular mission .', 'figure one standard object left white blue triangle emergent object right manikin dressed clothes potential shapes according competition rulebook circle semicircle quarter circle triangle rectangle pentagon star cross potential colors according competition rulebook white black red blue green purple brown orange targets detected minimum altitude 75 ft ideally within range 85 90 ft. classification target able determine localization signaling payload drop coordinates uav .', 'figure one standard object left white blue triangle emergent object right manikin dressed clothes potential shapes according competition rulebook circle semicircle quarter circle triangle rectangle pentagon star cross potential colors according competition rulebook white black red blue green purple brown orange targets detected minimum altitude 75 ft ideally within range 85 90 ft. classification target able determine localization signaling payload drop coordinates uav .', 'suas student unmanned aerial systems yearly competition particular mission . larger emphasis individuals able work project extended time 23 years solidify solid group future suas competition . marc cruz although want win competition time software lead also wanted ensure project longevity terms knowledge accumulated years competition . previous years lacked proper documentation available members transfer information current competition year .', 'looked various open source lidar implementations mits mader capable generating initial flight path creating spline followed goes around object .', '2.5.1 odlc trade studies 2.5.1.1 obc trade study obc trade study raspberry pi five coral accelerator raspberry pi four coral accelerator jetson orin nano criteria weight description grade weighted description grade weighted description grade weighted cost two 171 three six 170 three six 500 one two processing units five quadcore 64bit arm cortexa76 cpu one five cortexa72 arm v8 64bit soc 1.8ghz one five 1024core nvidia ampere architecture gpu 32 tensor cores 3.5 17.5 software support one yes 3.5 3.5 yes 3.5 3.5 yes 3.5 3.5 power consumption w three twelve two six eight three nine 1022 two six weight g one 48 3.5 3.5 47 3.5 3.5 176 two two average grade 2.6 2.8 2.4 weighted total 24 27 31 obc normalization chart grade one two three 3.5 cost 300 300 200 100 processing units reduced accuracy subpar processing subpar optimal semi optimal processing optimal processing software support software obsolescence software update date power consumption w thirteen twelve eight five weight g 200 200 100 50 2.5.1.1 camera trade study camera trade study oakd pro w poe imx378 sensor siyi zr10 rpi hq camera 6mm lens criteria weight description grade weighted description grade weighted description grade weighted cost one 599 one one 506 one one 75 3.5 3.5 weight g one 184 three three 381 one one 83.4 3.5 3.5 resolutionfps five 1080p60fps 4k30fps 3.5 17.5 2560 x 1440p30 3.5 17.5 2028 1080p50 2028 1520p40 1332 990p120 3.5 17.5 power consumption w three 2w 5.5 w three nine three w three nine 1.4 w 3.5 10.5 fov four hfov 95 vfov 70 3.5 fourteen hfovl 71.5 dfov 79.5 three twelve 63 three twelve average grade 2.8 2.3 3.4 weighted total 44.5 40.5 47 camera normalization chart grade one two three 3.5 cost 400 400 250 250 100 100 weight g 350 350 200 200 50 50 resolutionfps incapable high resolution 30 fps high resolution 30 fps power consumption w ten ten five five two two fov subpar optimal fov moderately optimal fov strongly optimal fov 2.5.2 obstacle avoidance trade study lidar trade study rplidar a2m8 livox mid40 rplidar a3 criteria weight description grade weighted description grade weighted description grade weighted cost one 319 three three 599 three three 599 three three detection range five 0.1 sixteen 3.5 17.5 260 3.5 17.5 eight two ten software support three yes 3.5 10.5 yes 3.5 10.5 yes 3.5 10.5 power consumption w four three three twelve ten one four three three twelve weight g two 190 two four 760 one two 190 two four average grade three 2.4 2.7 weighted total 47 37 39.5 lidar normalization chart grade one two three 3.5 cost 800 800 400 200 range six eight twelve twelve software support software obsolescence software update date power consumption w five four three two weight g 200 200 150 100 2.6 data collection dataset phase october december phase members focused working odlc mission task split models focused researching collecting required data specific machine learning model .', 'scripting images currently experimental work would completed winter break . shape model likely use yolo series remains determined winter break .', 'second approach creating finding rgb value dataset various rgb values labeled corresponding color .', 'mind extract rgb values shape alphanumeric two common values image . approach chose determine rgb values image move onto various methods making model . two approaches using finite range rgb values 0255 make range rgb value determines certain color . second approach creating finding rgb value dataset various rgb values labeled corresponding color . crossreferencing pixel values rgb equivalentthe color prevalent picture used determine overall color .', '2.7.1.1 training yolo models train custom dataset yolov5 need image file png jpeg .txt file contains class value xcoordinate ycoordinate center target width height target .', 'coordinates width height need normalized zero 1. way dividing xcoordinate width xmax ycoordinate height ymax xmax total width image minus one ymax total height image minus 1. image size 100 pxl x 200 pxl center bounding box xy 2677 width height bounding box wh 56 divide 26 99 five 99 well divide 77 199 six 199 well .', 'test validity synthetic dataset collected trained yolov5 custom dataset . model trained 750 training images plus 500 validation images 24000 images collected circle results promising . cause confusion matrix high possibly overfitting training model validation contain images similar . mind extract rgb values shape alphanumeric two common values image . approach chose determine rgb values image move onto various methods making model . two approaches using finite range rgb values 0255 make range rgb value determines certain color . second approach creating finding rgb value dataset various rgb values labeled corresponding color . crossreferencing pixel values rgb equivalentthe color prevalent picture used determine overall color . 2.7.1 uav image recognition trade study decision matrix methodologies study convolutional neural networks cnns faster rcnn yolo look criteria evaluation accuracy ability correctly identify classify objects . 2.7.1.1 training yolo models train custom dataset yolov5 need image file png jpeg .txt file contains class value xcoordinate ycoordinate center target width height target . class number must start zero add many empty values yaml file discuss later xcoordinate ycoordinate width height ex zero 0.815 0.777 0.0345 0.0392 phase testing make sure generate proper data used train yolo creating data training yolo next weeks . however 23000 images lot would take lot time testing purposes trained model 750 training images plus 500 validation images good time mention yolo takes data train . train yolo custom dataset need divide dataset three sections create three folders within dataset folder train val test train val mandatory test ignored good practice two subfolders images labels . train folder contain images corresponding annotations val validation folder contain less data validated every epochs number times train model training dataset lastly test contain even less data ran testing end . important note images folders different especially train val want validate trained model unique set images increase accuracy . also need yaml file contain location folder train val test addition number classes names classes . would attempt turn models tensorflow lite models equivalent form ran developer kit . nine j. redmon s. divvala r. girshick a. farhadi look unified realtime object detection proc .', 'start training need navigate yolov5 cloned folder command prompt place command python train.py imgsize 640 batchsize sixteen epochs 50 data datayour_yaml_file.yaml weights yolov5s.pt .', 'start training need navigate yolov5 cloned folder command prompt place command python train.py imgsize 640 batchsize sixteen epochs 50 data datayour_yaml_file.yaml weights yolov5s.pt .', 'scripting images currently experimental work would completed winter break . shape model likely use yolo series remains determined winter break .', 'suas competition software team fall 2023 final report authors marc cruz abdul kalam syed max gross joshua estrada jason mar josh ng ethan tarrer sarkis gafayan rubayet mujahid david jackson status done date sep 21 2023 relative links suas competition software team overview suas competition technical design document suas competition machine learning models 1. introduction two 2. fall 2023 progress three 2.1 timeline three 2.2 recruitment august september four 2.3 learning phase august october four 2.3.1 odlc four 2.3.2 obstacle avoidance five 2.4 odlc design five 2.5 hardware trade studies seven 2.5.1 odlc trade studies seven 2.5.1.1 obc trade study seven 2.5.1.1 camera trade study eight 2.5.2 obstacle avoidance trade study nine 2.6 data collection dataset phase october december ten 2.6.1 standardized object shape ml model ten 2.6.2 standardized object shape alphanumeric color ml model twelve 2.6.3 standardized object alphanumeric ml model thirteen 2.7 modeling phase october february thirteen 2.7.1 uav image recognition trade study decision matrix thirteen 2.7.1.1 training yolo models fifteen 3. reflectionmethod revisions seventeen 4. conclusionfuture work seventeen 5. references eighteen 1. introduction progress report software team suas 2024. document contains current progress project also adding new revisions suas 2025 software team consider increase odds cpp suas team win consistently . 2024 competition mission multiple package delivery companies tasked uas deliver packages customers . fix issues lack expertise future suas software members would begin recruitment spring 2024 allow new members time familiarize work capable contributing fall 2024. also reduces need recruitment next suas competition utilizes full semester work rather recruiting .']\n"
     ]
    }
   ],
   "source": [
    "print(train_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957bf9bd-ac9a-487a-9cfa-cc47eaf36012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['What is 2023?', 'Can you provide details about 2023?', 'Explain the significance of 2023.', 'What are the properties of 2023?'], ['What is yearly?', 'Can you provide details about yearly?', 'Explain the significance of yearly.', 'What are the properties of yearly?'], ['What is semicircle?', 'Can you provide details about semicircle?', 'Explain the significance of semicircle.', 'What are the properties of semicircle?'], ['What is pentagon?', 'Can you provide details about pentagon?', 'Explain the significance of pentagon.', 'What are the properties of pentagon?'], ['What is year?', 'Can you provide details about year?', 'Explain the significance of year.', 'What are the properties of year?'], ['What is mader?', 'Can you provide details about mader?', 'Explain the significance of mader.', 'What are the properties of mader?'], ['What is v8?', 'Can you provide details about v8?', 'Explain the significance of v8.', 'What are the properties of v8?'], ['What is winter?', 'Can you provide details about winter?', 'Explain the significance of winter.', 'What are the properties of winter?'], ['What is second?', 'Can you provide details about second?', 'Explain the significance of second.', 'What are the properties of second?'], ['What is rgb?', 'Can you provide details about rgb?', 'Explain the significance of rgb.', 'What are the properties of rgb?'], ['What is jpeg?', 'Can you provide details about jpeg?', 'Explain the significance of jpeg.', 'What are the properties of jpeg?'], ['What is pxl?', 'Can you provide details about pxl?', 'Explain the significance of pxl.', 'What are the properties of pxl?'], ['What is val known for?', 'What are the contributions of val?', 'Can you provide some information about val?', 'Tell me about val.'], ['What is weights known for?', 'What are the contributions of weights?', 'Can you provide some information about weights?', 'Tell me about weights.'], ['What is yolov5s.pt known for?', 'What are the contributions of yolov5s.pt?', 'Can you provide some information about yolov5s.pt?', 'Tell me about yolov5s.pt.'], ['What is winter?', 'Can you provide details about winter?', 'Explain the significance of winter.', 'What are the properties of winter?'], ['What is 2024?', 'Can you provide details about 2024?', 'Explain the significance of 2024.', 'What are the properties of 2024?']]\n"
     ]
    }
   ],
   "source": [
    "print(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8053503e-82d7-4e88-af38-75830368bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'start_index': 36, 'end_index': 40}, {'start_index': 36, 'end_index': 40}, {'start_index': 36, 'end_index': 40}, {'start_index': 36, 'end_index': 40}, {'start_index': 36, 'end_index': 40}], [{'start_index': 37, 'end_index': 43}, {'start_index': 37, 'end_index': 43}, {'start_index': 37, 'end_index': 43}, {'start_index': 37, 'end_index': 43}, {'start_index': 37, 'end_index': 43}], [{'start_index': 153, 'end_index': 163}, {'start_index': 153, 'end_index': 163}, {'start_index': 153, 'end_index': 163}, {'start_index': 153, 'end_index': 163}, {'start_index': 153, 'end_index': 163}], [{'start_index': 198, 'end_index': 206}, {'start_index': 198, 'end_index': 206}, {'start_index': 198, 'end_index': 206}, {'start_index': 198, 'end_index': 206}, {'start_index': 198, 'end_index': 206}], [{'start_index': 37, 'end_index': 41}, {'start_index': 37, 'end_index': 41}, {'start_index': 37, 'end_index': 41}, {'start_index': 37, 'end_index': 41}, {'start_index': 37, 'end_index': 41}], [{'start_index': 54, 'end_index': 59}, {'start_index': 54, 'end_index': 59}, {'start_index': 54, 'end_index': 59}, {'start_index': 54, 'end_index': 59}, {'start_index': 54, 'end_index': 59}], [{'start_index': 378, 'end_index': 380}, {'start_index': 378, 'end_index': 380}, {'start_index': 378, 'end_index': 380}, {'start_index': 378, 'end_index': 380}, {'start_index': 378, 'end_index': 380}], [{'start_index': 61, 'end_index': 67}, {'start_index': 61, 'end_index': 67}, {'start_index': 61, 'end_index': 67}, {'start_index': 61, 'end_index': 67}, {'start_index': 61, 'end_index': 67}], [{'start_index': 0, 'end_index': 6}, {'start_index': 0, 'end_index': 6}, {'start_index': 0, 'end_index': 6}, {'start_index': 0, 'end_index': 6}, {'start_index': 0, 'end_index': 6}], [{'start_index': 13, 'end_index': 16}, {'start_index': 13, 'end_index': 16}, {'start_index': 13, 'end_index': 16}, {'start_index': 13, 'end_index': 16}, {'start_index': 13, 'end_index': 16}], [{'start_index': 77, 'end_index': 81}, {'start_index': 77, 'end_index': 81}, {'start_index': 77, 'end_index': 81}, {'start_index': 77, 'end_index': 81}, {'start_index': 77, 'end_index': 81}], [{'start_index': 190, 'end_index': 193}, {'start_index': 190, 'end_index': 193}, {'start_index': 190, 'end_index': 193}, {'start_index': 190, 'end_index': 193}, {'start_index': 190, 'end_index': 193}], [{'start_index': 5, 'end_index': 8}, {'start_index': 5, 'end_index': 8}, {'start_index': 5, 'end_index': 8}, {'start_index': 5, 'end_index': 8}, {'start_index': 5, 'end_index': 8}], [{'start_index': 164, 'end_index': 171}, {'start_index': 164, 'end_index': 171}, {'start_index': 164, 'end_index': 171}, {'start_index': 164, 'end_index': 171}, {'start_index': 164, 'end_index': 171}], [{'start_index': 172, 'end_index': 182}, {'start_index': 172, 'end_index': 182}, {'start_index': 172, 'end_index': 182}, {'start_index': 172, 'end_index': 182}, {'start_index': 172, 'end_index': 182}], [{'start_index': 61, 'end_index': 67}, {'start_index': 61, 'end_index': 67}, {'start_index': 61, 'end_index': 67}, {'start_index': 61, 'end_index': 67}, {'start_index': 61, 'end_index': 67}], [{'start_index': 1258, 'end_index': 1263}, {'start_index': 1258, 'end_index': 1263}, {'start_index': 1258, 'end_index': 1263}, {'start_index': 1258, 'end_index': 1263}, {'start_index': 1258, 'end_index': 1263}]]\n"
     ]
    }
   ],
   "source": [
    "print(train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8490f1a2-e8dc-4151-8ecb-d377a30cec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_train_contexts = train_contexts[:]\n",
    "backup_train_questions = train_questions[:]\n",
    "backup_train_answers = train_answers[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d7424cc-2f37-453d-ba2c-04dea1d5bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts = backup_train_contexts[:]\n",
    "train_questions = backup_train_questions[:]\n",
    "train_answers = backup_train_answers[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea1614d1-ed2b-41e2-b9c1-280be0d119d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "torch.Size([12, 512])\n",
      "torch.Size([12, 512])\n",
      "torch.Size([12, 512])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_end_positions_tensor[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)    \u001b[38;5;66;03m# Print shape of the first tensor in the list\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Create a PyTorch dataset\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_type_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_start_positions_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_end_positions_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Define your DataLoader\u001b[39;00m\n\u001b[1;32m     77\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ArchiveReader/lib/python3.11/site-packages/torch/utils/data/dataset.py:203\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    204\u001b[0m         tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    205\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "# Restore the original data\n",
    "train_contexts = backup_train_contexts[:]\n",
    "train_questions = backup_train_questions[:]\n",
    "train_answers = backup_train_answers[:]\n",
    "\n",
    "# Convert contexts and questions to strings\n",
    "train_contexts = [str(context) for context in train_contexts]\n",
    "train_questions = [str(question) for question in train_questions]\n",
    "\n",
    "# Tokenize inputs\n",
    "inputs = tokenizer(train_contexts, \n",
    "                   train_questions, \n",
    "                   padding=True, \n",
    "                   truncation=True, \n",
    "                   return_tensors='pt',\n",
    "                   max_length=512, \n",
    "                   add_special_tokens=True)\n",
    "\n",
    "# Convert answers to answer indices\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "for answers in train_answers:\n",
    "    entity_start_positions = []\n",
    "    entity_end_positions = []\n",
    "    for ans in answers:\n",
    "        start_char = ans['start_index']\n",
    "        end_char = ans['end_index']\n",
    "        # Find corresponding token indices using offsets\n",
    "        start_token = inputs.char_to_token(0, start_char)\n",
    "        end_token = inputs.char_to_token(0, end_char)\n",
    "        entity_start_positions.append(start_token)\n",
    "        entity_end_positions.append(end_token)\n",
    "    start_positions.append(entity_start_positions)\n",
    "    end_positions.append(entity_end_positions)\n",
    "\n",
    "# Filter out None values from start_positions and end_positions\n",
    "start_positions = [[pos for pos in sublist if pos is not None] for sublist in start_positions]\n",
    "end_positions = [[pos for pos in sublist if pos is not None] for sublist in end_positions]\n",
    "\n",
    "# Filter out samples where end positions are empty\n",
    "non_empty_end_positions_indices = [i for i, pos in enumerate(end_positions) if len(pos) > 0]\n",
    "\n",
    "# Use the filtered indices to select corresponding tensors\n",
    "filtered_inputs = {key: value[non_empty_end_positions_indices] for key, value in inputs.items()}\n",
    "filtered_start_positions_tensor = [start_positions_tensor[i] for i in non_empty_end_positions_indices]\n",
    "filtered_end_positions_tensor = [end_positions_tensor[i] for i in non_empty_end_positions_indices]\n",
    "\n",
    "# Check the lengths of the tensors\n",
    "num_samples = len(filtered_inputs['input_ids'])\n",
    "assert len(filtered_inputs['token_type_ids']) == num_samples, \"Size mismatch between token_type_ids and input_ids\"\n",
    "assert len(filtered_inputs['attention_mask']) == num_samples, \"Size mismatch between attention_mask and input_ids\"\n",
    "assert len(filtered_start_positions_tensor) == num_samples, \"Size mismatch between start_positions and input_ids\"\n",
    "assert len(filtered_end_positions_tensor) == num_samples, \"Size mismatch between end_positions and input_ids\"\n",
    "\n",
    "print(len(filtered_inputs['input_ids']))\n",
    "print(len(filtered_inputs['token_type_ids']))\n",
    "print(len(filtered_inputs['attention_mask']))\n",
    "print(len(filtered_start_positions_tensor))\n",
    "print(len(filtered_end_positions_tensor))\n",
    "\n",
    "print(filtered_inputs['input_ids'].shape)\n",
    "print(filtered_inputs['token_type_ids'].shape)\n",
    "print(filtered_inputs['attention_mask'].shape)\n",
    "print(filtered_start_positions_tensor[0].shape)  # Print shape of the first tensor in the list\n",
    "print(filtered_end_positions_tensor[0].shape)    # Print shape of the first tensor in the list\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "train_dataset = TensorDataset(filtered_inputs['input_ids'], filtered_inputs['token_type_ids'], filtered_inputs['attention_mask'],\n",
    "                              *filtered_start_positions_tensor, *filtered_end_positions_tensor)\n",
    "\n",
    "# Define your DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, token_type_ids, attention_mask, start_positions, end_positions = batch\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = loss_fn(outputs.start_logits, start_positions.view(-1)) + loss_fn(outputs.end_logits, end_positions.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('bert_qa_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cb266-1ebf-48ac-a15e-8d20848849ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
